<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=zh><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Tensor concept - sh1marin's blog</title><meta name=theme-color><meta name=description content="Tensor 的一些概念 不论是 Pytorch 还是 TensorFlow，Tensor 的表现都和 Numpy 的 Array 类似。 在 PyTorch 里一个 2 维的 Tensor 可以用这样的方式来一探究竟： tensor = torch.ones(4, 4) print(f&#34;First row: {tensor[0]}&#34;)"><meta name=author content="sh1marin"><link rel="preload stylesheet" as=style href=https://blog.sh1mar.in/main.min.css><script defer src=https://blog.sh1mar.in/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=https://blog.sh1mar.in/theme.png><link rel=preload as=image href=https://blog.sh1mar.in/github.svg><link rel=preload as=image href=https://blog.sh1mar.in/rss.svg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<link rel=icon href=https://blog.sh1mar.in/favicon.ico><link rel=apple-touch-icon href=https://blog.sh1mar.in/apple-touch-icon.png><meta name=generator content="Hugo 0.112.3"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Tensor concept"><meta property="og:description" content="Tensor 的一些概念 不论是 Pytorch 还是 TensorFlow，Tensor 的表现都和 Numpy 的 Array 类似。 在 PyTorch 里一个 2 维的 Tensor 可以用这样的方式来一探究竟： tensor = torch.ones(4, 4) print(f&#34;First row: {tensor[0]}&#34;)"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.sh1mar.in/post/mlir/tensor/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-06-12T00:00:00+08:00"><meta property="article:modified_time" content="2023-06-12T00:00:00+08:00"><meta itemprop=name content="Tensor concept"><meta itemprop=description content="Tensor 的一些概念 不论是 Pytorch 还是 TensorFlow，Tensor 的表现都和 Numpy 的 Array 类似。 在 PyTorch 里一个 2 维的 Tensor 可以用这样的方式来一探究竟： tensor = torch.ones(4, 4) print(f&#34;First row: {tensor[0]}&#34;)"><meta itemprop=datePublished content="2023-06-12T00:00:00+08:00"><meta itemprop=dateModified content="2023-06-12T00:00:00+08:00"><meta itemprop=wordCount content="2632"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Tensor concept"><meta name=twitter:description content="Tensor 的一些概念 不论是 Pytorch 还是 TensorFlow，Tensor 的表现都和 Numpy 的 Array 类似。 在 PyTorch 里一个 2 维的 Tensor 可以用这样的方式来一探究竟： tensor = torch.ones(4, 4) print(f&#34;First row: {tensor[0]}&#34;)"></head><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center"><div class="relative z-50 mr-auto flex items-center"><a class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold" href=https://blog.sh1mar.in>sh1marin's blog</a><div class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6"><a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav><nav class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-12 lg:mt-0 lg:items-center lg:space-x-6"><a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/Avimitin target=_blank rel=me>github</a>
<a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./rss.svg) href=https://blog.sh1mar.in/index.xml target=_blank rel=alternate>rss</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"><article><header class=mb-16><h1 class="!my-0 pb-2.5">Tensor concept</h1><div class="text-sm antialiased opacity-60"><time>Jun 12, 2023</time>
<span class=mx-1>&#183;</span>
<span>sh1marin</span></div></header><section><h2 id=tensor-的一些概念>Tensor 的一些概念</h2><p>不论是 Pytorch 还是 TensorFlow，Tensor 的表现都和
<a href=https://numpy.org/doc/stable/user/basics.indexing.html>Numpy 的 Array</a> 类似。</p><p>在 PyTorch 里一个 2 维的 Tensor 可以用这样的方式来一探究竟：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>tensor <span style=color:#fe8019>=</span> torch<span style=color:#fe8019>.</span>ones(<span style=color:#d3869b>4</span>, <span style=color:#d3869b>4</span>)
</span></span><span style=display:flex><span><span style=color:#fabd2f>print</span>(<span style=color:#b8bb26>f</span><span style=color:#b8bb26>&#34;First row: </span><span style=color:#b8bb26>{</span>tensor[<span style=color:#d3869b>0</span>]<span style=color:#b8bb26>}</span><span style=color:#b8bb26>&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#fabd2f>print</span>(<span style=color:#b8bb26>f</span><span style=color:#b8bb26>&#34;First column: </span><span style=color:#b8bb26>{</span>tensor[:, <span style=color:#d3869b>0</span>]<span style=color:#b8bb26>}</span><span style=color:#b8bb26>&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#fabd2f>print</span>(<span style=color:#b8bb26>f</span><span style=color:#b8bb26>&#34;Last column: </span><span style=color:#b8bb26>{</span>tensor[<span style=color:#fe8019>...</span>, <span style=color:#fe8019>-</span><span style=color:#d3869b>1</span>]<span style=color:#b8bb26>}</span><span style=color:#b8bb26>&#34;</span>)
</span></span><span style=display:flex><span>tensor[:,<span style=color:#d3869b>1</span>] <span style=color:#fe8019>=</span> <span style=color:#d3869b>0</span>
</span></span><span style=display:flex><span><span style=color:#fabd2f>print</span>(tensor)
</span></span></code></pre></div><p>Output:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-text data-lang=text><span style=display:flex><span>First row: tensor([1., 1., 1., 1.])
</span></span><span style=display:flex><span>First column: tensor([1., 1., 1., 1.])
</span></span><span style=display:flex><span>Last column: tensor([1., 1., 1., 1.])
</span></span><span style=display:flex><span>tensor([[1., 0., 1., 1.],
</span></span><span style=display:flex><span>        [1., 0., 1., 1.],
</span></span><span style=display:flex><span>        [1., 0., 1., 1.],
</span></span><span style=display:flex><span>        [1., 0., 1., 1.]])
</span></span></code></pre></div><p>torch.ones 创建了一个全是浮点数 1 的 4x4 的矩阵。
在 Python 里 <code>Array[i, j]</code> 只是 <code>Array[(i, j)]</code> 的语法糖，
所以这里可以很清晰的理解 <code>First column</code>，即第一列是怎么被索引的。</p><p>而在 TensorFlow 里的 tensor 都是 Immutable 的，
一切看起来像是更新的操作，实际上都会创建新的 tensor。</p><p>MLIR 的 tensor 概念上和 TensorFlow 的 tensor 比较相近。
在 TensorFlow 里，有常量 tensor：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rank_0_tensor <span style=color:#fe8019>=</span> tf<span style=color:#fe8019>.</span>constant(<span style=color:#d3869b>4</span>)
</span></span></code></pre></div><p>这是一个没有任何轴（没有行列）的，纯常量数值。
而一个 rank 1，只有一个轴的 tensor，表现上则和向量相近：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rank_1_tensor <span style=color:#fe8019>=</span> tf<span style=color:#fe8019>.</span>constant([<span style=color:#d3869b>2.0</span>, <span style=color:#d3869b>3.0</span>, <span style=color:#d3869b>4.0</span>])
</span></span></code></pre></div><p>一个带有两个轴，具有行列的矩阵，则是一个嵌套的数组：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rank_2_tensor <span style=color:#fe8019>=</span> tf<span style=color:#fe8019>.</span>constant([[<span style=color:#d3869b>1</span>, <span style=color:#d3869b>2</span>],
</span></span><span style=display:flex><span>                             [<span style=color:#d3869b>3</span>, <span style=color:#d3869b>4</span>],
</span></span><span style=display:flex><span>                             [<span style=color:#d3869b>5</span>, <span style=color:#d3869b>6</span>]], dtype<span style=color:#fe8019>=</span>tf<span style=color:#fe8019>.</span>float16)
</span></span></code></pre></div><p>这里同时还设置了 dtype 数据类型为 float16。</p><p>tensor 不仅限于只有 0-2 个轴，一个向量可以有更多的轴，比如三个轴来表现一个立方体：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rank_3_tensor <span style=color:#fe8019>=</span> tf<span style=color:#fe8019>.</span>constant([
</span></span><span style=display:flex><span>  [[<span style=color:#d3869b>0</span>, <span style=color:#d3869b>1</span>, <span style=color:#d3869b>2</span>, <span style=color:#d3869b>3</span>, <span style=color:#d3869b>4</span>],
</span></span><span style=display:flex><span>   [<span style=color:#d3869b>5</span>, <span style=color:#d3869b>6</span>, <span style=color:#d3869b>7</span>, <span style=color:#d3869b>8</span>, <span style=color:#d3869b>9</span>]],
</span></span><span style=display:flex><span>  [[<span style=color:#d3869b>10</span>, <span style=color:#d3869b>11</span>, <span style=color:#d3869b>12</span>, <span style=color:#d3869b>13</span>, <span style=color:#d3869b>14</span>],
</span></span><span style=display:flex><span>   [<span style=color:#d3869b>15</span>, <span style=color:#d3869b>16</span>, <span style=color:#d3869b>17</span>, <span style=color:#d3869b>18</span>, <span style=color:#d3869b>19</span>]],
</span></span><span style=display:flex><span>  [[<span style=color:#d3869b>20</span>, <span style=color:#d3869b>21</span>, <span style=color:#d3869b>22</span>, <span style=color:#d3869b>23</span>, <span style=color:#d3869b>24</span>],
</span></span><span style=display:flex><span>   [<span style=color:#d3869b>25</span>, <span style=color:#d3869b>26</span>, <span style=color:#d3869b>27</span>, <span style=color:#d3869b>28</span>, <span style=color:#d3869b>29</span>]],])
</span></span></code></pre></div><p>上述例子是个 3x2x5 的，三个轴（或者称三个维度）的 tensor。
可以观测到，在描述维度的时候，是从外向内的。</p><p>一般普通的 tensor 要求所有的边都能成直角，
即在某一个维度或者某一个轴上的数值的数量都应该相等。
但也有特例，比如稀疏矩阵 Sparse Tensor。</p><p>在 TensorFlow 里有这么几个词汇用来描述 tensor。
<em><strong>Shape</strong></em> 通常是个数组，用来描述所有轴上元素的数量，比如上述 <code>rank_3_tensor</code> 的 shape 是 <code>[3,2,5]</code>。
<em><strong>Rank</strong></em> 用来描述 tensor 有几个轴，一个常量 rank 为 0，一个向量的 rank 是 1&mldr;
<em><strong>Axis</strong></em> 或者 <em><strong>Dimension</strong></em> 用来指定 tensor 的某一特定的维度。
<em><strong>dtype</strong></em> 通常指 tensor 内数据的类型。
最后 <em><strong>Size</strong></em> 用来描述 tensor 里所有元素的数量。</p><p>有一个很重要的概念点是，二维 tensor 并不意味 rank 2 tensor，一个 rank 2 的 tensor
很有可能并不用来描述二维空间。</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rank_4_tensor <span style=color:#fe8019>=</span> tf<span style=color:#fe8019>.</span>zeros([<span style=color:#d3869b>3</span>, <span style=color:#d3869b>2</span>, <span style=color:#d3869b>4</span>, <span style=color:#d3869b>5</span>])
</span></span></code></pre></div><p>在上述的例子里，rank_4_tensor 是一个 <code>rank</code> 为 <code>4</code> 的 tensor，其内部所有的元素的值都是 0。
<code>shape</code> 是 <code>[3, 2, 4, 5]</code>，有 4 个 <code>axis</code>，其中 <code>axis 1</code> 的长度是 2，
size 则是 <code>3*2*4*5 = 120</code>。</p><p>TODO: 什么是 batch & feature axis</p><h3 id=broadcasting>Broadcasting</h3><p>broadcasting 指在某些条件下，当对较小的 tensor 进行组合操作时，
较小的 tensor 会被自动 &ldquo;拉长 &ldquo;以适应较大的 tensor。
比如将一个常量与向量做乘积，常量会先变成同等长度的向量，然后再做乘积。
而一个 shape 为 <code>[3, 1]</code> 的 tensor 和 shape 为 <code>[1, 4]</code> 的向量做乘积运算
会最后生成一个 <code>[3, 4]</code> 的 tensor。</p><h2 id=mlir>MLIR</h2><p>在 MLIR 里，tensor 是个原生的类型，存在于 builtin Dialect 里，而对 tensor 的操作则放在 tensor Dialect 里。
这些都是独立的 tensor 创建和修改操作，不会与其他 Dialect 存在耦合。
一些对 tensor 的特殊计算操作会放到其他的 Dialect 里。</p><p>MLIR 里的 tensor 支持各类元素类型，可以用来表达 MLIR 里的各种概念，包括但不限于：</p><ul><li>用来表达适用于高性能计算的大型且密集的元素集合</li><li>使用 sparse_tensor.encoding 来表达适用于高性能计算的大型稀疏数据集合</li><li>用小型一维的 tensor 存储 index 类型，来表达 shape Dialect 中的 shapes</li><li>用于表达字符串或者可变的元素集合</li></ul><p>在 MLIR 里，有两种 tensor 类型，一种是 RankedTensorType 另一种是 UnrankedTensorType。
RankedTensorType 的 Rank 是固定的，但 Axis(或者说 Dimension) 是可以动态的。
表达一个 Rank-2 且固定 Shape 的 Tensor 可以用 <code>tensor&lt;3x2xi32></code>，
一个 shape 是 <code>[3,2]</code> 的，dtype 是 i32 的 tensor。
也可以用 <code>tensor&lt;?x?x?xi32></code> 来表达 Rank-3 但动态的 tensor。
除此之外，MLIR 的 Tensor 也可以用来表达常量，
比如 <code>tensor&lt;f32></code> 表达一个 Rank 0，元素类型为 f32 的常量。
维度的长度也可以是 0，用类似于 <code>tensor&lt;0x4xf32></code> 的类型来表达。
同时这种特殊的用 <code>i x j</code> 来表达 shape 的方式，也限制住不能使用十六进制来定义 axis 的值。</p><p>UnrankedTensorType 则用来表达动态 rank 的 tensor 类型，用 <code>tensor&lt;*xdtype></code> 来表示，
比如 <code>tensor&lt;*xi32></code>。</p><p>如此灵活的抽象的 tensor 类型是 MLIR 设计的一部分，
使用者不应该操心一个 tensor 的机器表示应该是怎么样的，
这些抽象操作到机器层级的映射最后会被 lower 到 memref Dialect 上，
在那一个层级才有相对底层的缓存访问实现。</p><p>有关 tensor 相关操作的文档，可以在 <a href=https://mlir.llvm.org/docs/Dialects/TensorOps/>tensor Dialect</a>
里查看。</p><h3 id=bufferization>Bufferization</h3><p>在 MLIR 里，将 tensor 的操作转换到 memref 操作的这么一个过程被称作为 <em><strong>Bufferization</strong></em>。
最早的时候 MLIR 是在 tensor 到 memref Dialect 转换的时候做 bufferization，但为了
减少内存 alloc 和复制，后来改成了用单个 pass (<em><strong>One-Shot Bufferization</strong></em>) 来一次性 bufferize
整个程序。</p><p><em><strong>Bufferization</strong></em> 进程有两个目标：1. 尽量少的申请内存，2. 尽量少的复制内存。
为了实现这两个目标，bufferize 可能会为了复用内存而产生很多很复杂的算法。
给定一个 tensor 的操作结果，<em><strong>Bufferization</strong></em> 需要选择一个 memref buffer 来存储。
最安全的做法是给所有的操作都生成一个新的 buffer，但这肯定是不符合预期的。
而为了安全复用缓存，使得操作不会覆写一些仍然需要的数据，就使得决策变得相当困难复杂。
除此之外，也有一些复杂的情况会影响 <em><strong>Bufferization</strong></em> 进程的决策，
比如有时候复制内存的开销可能小于重新计算的开销，或者有些平台不支持申请新内存。</p><p>为了简化这个问题，<em><strong>One-Shot Bufferization</strong></em> 只对 <em>destination-passing style</em> 的操作做 <em>Bufferization</em>。
什么是 <em>destination-passing style</em> 呢，考虑以下这个例子：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mlir data-lang=mlir><span style=display:flex><span>%0 = <span style=color:#fabd2f>tensor</span>.insert %cst into %t[%idx] : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt;
</span></span></code></pre></div><p><code>tensor.insert</code> 复制一份给定的 tensor，将给定常量插入 index，并返回这个复制的 tensor。
在上述例子中， <code>%0</code> 是返回值，<code>%csr</code> 和 <code>%t</code> 是 <code>tensor.insert</code> 的操作数，
因为 <code>%csr</code> 是常数，而 <code>%t</code> 是个 tensor，因此此处 <code>%t</code> 就是 destination，
在考虑如何存放 <code>%0</code> 时就只有两个选择：</p><ol><li>创建一个新的 buffer</li><li>复用操作数 <code>%t</code> 的 buffer</li></ol><p>在程序执行的过程中可能会存在更多的无用垃圾 buffer 可以拿来复用，
但复用那些内存会引入更加巨量的问题。</p><p>如果是不符合 <em>destination-passing style</em> 的操作，<em>Bufferization</em> 会为这些 tensor 开辟新的内存。</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mlir data-lang=mlir><span style=display:flex><span>%0 = <span style=color:#fabd2f>tensor</span>.generate %sz {
</span></span><span style=display:flex><span><span style=color:#fb4934>^bb0</span>(%i : <span style=color:#fe8019>index</span>):
</span></span><span style=display:flex><span>  %cst = arith.<span style=color:#fabd2f>constant</span> <span style=color:#d3869b>0.0</span> : <span style=color:#fe8019>f32</span>
</span></span><span style=display:flex><span>  <span style=color:#fabd2f>tensor</span>.yield %cst : <span style=color:#fe8019>f32</span>
</span></span><span style=display:flex><span>} : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt;
</span></span></code></pre></div><p>比如此处 <code>tensor.generate</code> 只接受一个 <code>block</code>，并没有任何的 &ldquo;<em>destination</em>"，
因此 <em>Bufferization</em> 会为返回的 tensor 开辟新的内存。
也可以用 <code>linalg.generic</code> 改写成 <em>destination-passing style</em>：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mlir data-lang=mlir><span style=display:flex><span>#map = affine_map&lt;(i) -&gt; (i)&gt;
</span></span><span style=display:flex><span>%0 = linalg.generic {<span style=color:#fb4934>indexing_maps =</span> [#map], <span style=color:#fb4934>iterator_types =</span> [<span style=color:#b8bb26>&#34;parallel&#34;</span>]}
</span></span><span style=display:flex><span>                    outs(%t : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt;) {
</span></span><span style=display:flex><span>  <span style=color:#fb4934>^bb0</span>(%arg0 : <span style=color:#fe8019>f32</span>):
</span></span><span style=display:flex><span>    %cst = arith.<span style=color:#fabd2f>constant</span> <span style=color:#d3869b>0.0</span> : <span style=color:#fe8019>f32</span>
</span></span><span style=display:flex><span>    linalg.yield %cst : <span style=color:#fe8019>f32</span>
</span></span><span style=display:flex><span>} -&gt; <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt;
</span></span></code></pre></div><p>这里很明显的能看到 <code>%t</code> 就是 destination，但同时也能看出一点奇怪的地方，
<code>outs</code> 里的参数似乎就是用来被覆写的，为什么还要专门传入一个参数呢？
可以看下下面这个例子:</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mlir data-lang=mlir><span style=display:flex><span>%t = <span style=color:#fabd2f>tensor</span>.extract_slice %s [%idx] [%sz] [<span style=color:#d3869b>1</span>] : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt; to <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt;
</span></span><span style=display:flex><span>%0 = linalg.generic ... outs(%t) { ... } -&gt; <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt;
</span></span><span style=display:flex><span>%1 = <span style=color:#fabd2f>tensor</span>.insert_slice %0 into %s [%idx] [%sz] [<span style=color:#d3869b>1</span>]
</span></span><span style=display:flex><span>    : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt; into <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt;
</span></span></code></pre></div><p><code>tensor.extract_slice</code> 取出一小段切片，之后会被 <em>bufferize</em> 到 <code>memref.subview</code>。
然后把这个切片 <code>%t</code> 传入到 <code>linalg.generic</code> 的 outs 里，
最后 <code>tensor.insert_slice</code> 则会将 <code>%0</code> 插入被取出 slice 的原 tensor 里。
由这个例子可以看出一个设计传入特定 <code>out</code> 的原因：可以用来指定覆写哪一段的内存。</p><p>除此之外，值得一提的是，<code>tensor.insert_slice</code> 最后会被优化掉。
有 SSA 的加持，MLIR 能发现源操作数来源于目标操作数，于是这些操作能被消除掉。</p></section><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]"><a class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://blog.sh1mar.in/post/nix/bump-vector-llvm/><span class=mr-1.5>←</span><span>Using latest LLVM in nix</span></a>
<a class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://blog.sh1mar.in/post/mlir/sparse-compiler-pass-sparsification-and-bufferization-pass/><span>MLIR Sparse Compiler - SparsificationAndBufferizationPass</span><span class=ml-1.5>→</span></a></nav></article></main><footer class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"><div class=mr-auto>&copy; 2024
<a class=link href=https://blog.sh1mar.in>sh1marin's blog</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>Powered by Hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>✎ Paper</a></footer></body></html>