<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=zh><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>MLIR Sparse Compiler - SparsificationAndBufferizationPass - sh1marin's blog</title><meta name=theme-color><meta name=description content="如我上上篇博客所提，MLIR 实现稀疏算法的方式是靠类似于 TACO 的 Sparse Compiler 来将普通操作转换成稀疏矩阵操作。 为了不引入新的操作和语义，MLIR sparse_tensor 只引入了"><meta name=author content="sh1marin"><link rel="preload stylesheet" as=style href=https://blog.sh1mar.in/main.min.css><script defer src=https://blog.sh1mar.in/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=https://blog.sh1mar.in/theme.png><link rel=preload as=image href=https://blog.sh1mar.in/github.svg><link rel=preload as=image href=https://blog.sh1mar.in/rss.svg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<link rel=icon href=https://blog.sh1mar.in/favicon.ico><link rel=apple-touch-icon href=https://blog.sh1mar.in/apple-touch-icon.png><meta name=generator content="Hugo 0.112.3"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="MLIR Sparse Compiler - SparsificationAndBufferizationPass"><meta property="og:description" content="如我上上篇博客所提，MLIR 实现稀疏算法的方式是靠类似于 TACO 的 Sparse Compiler 来将普通操作转换成稀疏矩阵操作。 为了不引入新的操作和语义，MLIR sparse_tensor 只引入了"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.sh1mar.in/post/mlir/sparse-compiler-pass-sparsification-and-bufferization-pass/"><meta property="article:section" content="post"><meta property="article:published_time" content="2023-06-05T00:00:00+08:00"><meta property="article:modified_time" content="2023-06-05T00:00:00+08:00"><meta itemprop=name content="MLIR Sparse Compiler - SparsificationAndBufferizationPass"><meta itemprop=description content="如我上上篇博客所提，MLIR 实现稀疏算法的方式是靠类似于 TACO 的 Sparse Compiler 来将普通操作转换成稀疏矩阵操作。 为了不引入新的操作和语义，MLIR sparse_tensor 只引入了"><meta itemprop=datePublished content="2023-06-05T00:00:00+08:00"><meta itemprop=dateModified content="2023-06-05T00:00:00+08:00"><meta itemprop=wordCount content="1703"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="MLIR Sparse Compiler - SparsificationAndBufferizationPass"><meta name=twitter:description content="如我上上篇博客所提，MLIR 实现稀疏算法的方式是靠类似于 TACO 的 Sparse Compiler 来将普通操作转换成稀疏矩阵操作。 为了不引入新的操作和语义，MLIR sparse_tensor 只引入了"></head><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center"><div class="relative z-50 mr-auto flex items-center"><a class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold" href=https://blog.sh1mar.in>sh1marin's blog</a><div class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6"><a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav><nav class="mt-12 flex justify-center space-x-10 dark:invert lg:ml-12 lg:mt-0 lg:items-center lg:space-x-6"><a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/Avimitin target=_blank rel=me>github</a>
<a class="h-8 w-8 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./rss.svg) href=https://blog.sh1mar.in/index.xml target=_blank rel=alternate>rss</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"><article><header class=mb-16><h1 class="!my-0 pb-2.5">MLIR Sparse Compiler - SparsificationAndBufferizationPass</h1><div class="text-sm antialiased opacity-60"><time>Jun 5, 2023</time>
<span class=mx-1>&#183;</span>
<span>sh1marin</span></div></header><section><p>如我<a href=../sparsity/compiler.md>上上篇博客</a>所提，MLIR 实现稀疏算法的方式是靠类似于 TACO 的 Sparse Compiler 来将普通操作转换成稀疏矩阵操作。
为了不引入新的操作和语义，MLIR sparse_tensor 只引入了一些必要的 operation 和 attribute，写 sparse 算法的
基础数据结构和算法操作还是用的 tensor type 和 linalg Dialect。MLIR 用 <a href=https://mlir.llvm.org/docs/PatternRewriter/>Pattern Rewriter</a>
在 Pass Pipeline 的时候将这些原本对 Dense 的操作改写成 Sparse 的操作，以此来实现对用户完全透明不可见的 Sparsity Transform.</p><p>这篇博客从 sparse compiler pipeline 的注册作为入口，自顶向下的看 sparse_tensor Dialect 是怎么实现 sparse compiler 的。</p><p>首先从 sparse tensor 的 pipeline 注册开始看</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mlir data-lang=mlir><span style=display:flex><span><span style=color:#928374;font-style:italic>// mlir/lib/Dialect/SparseTensor/Pipelines/SparseTensorPipelines.cpp
</span></span></span><span style=display:flex><span><span style=color:#928374;font-style:italic></span>
</span></span><span style=display:flex><span>void mlir::sparse_tensor::registerSparseTensorPipelines() {
</span></span><span style=display:flex><span>  PassPipelineRegistration&lt;SparseCompilerOptions&gt;(
</span></span><span style=display:flex><span>      <span style=color:#b8bb26>&#34;sparse-compiler&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#b8bb26>&#34;The standard pipeline for taking sparsity-agnostic IR using the&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#b8bb26>&#34; sparse-tensor type, and lowering it to LLVM IR with concrete&#34;</span>
</span></span><span style=display:flex><span>      <span style=color:#b8bb26>&#34; representations and algorithms for sparse tensors.&#34;</span>,
</span></span><span style=display:flex><span>      buildSparseCompiler);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><code>PassPipelineRegistration</code> 是个全局的 Pass Pipeline 注册器，这里注册了一系列的 <code>SparseCompilerOptions</code>，
并传递了 <code>buildSparseCompiler()</code> 函数指针用来构造 Sparse Compiler。<code>buildSparseCompiler()</code> 函数本身
只是个 Pass 注册函数，函数接收了一个 <code>OpPassManager</code> 并在里面注册从向量化，IR 规范化的 Pass
到 lowering 的 Pass。所有的 Pass 都会通过 <code>createXXXPass()</code> 函数将具体的 Pass 类型 implicit 的 upcast
到父类型 <code>Pass</code> 上。比如 <code>SparseGPUCodegenPass</code> 的注册函数是先创建了一个 <code>SparseGPUCodegenPass</code> 的
unique pointer，然后在 return 语句 cast 到父类型 <code>Pass</code> 上。</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c++ data-lang=c++><span style=display:flex><span>std<span style=color:#fe8019>::</span>unique_ptr<span style=color:#fe8019>&lt;</span>Pass<span style=color:#fe8019>&gt;</span> mlir<span style=color:#fe8019>::</span>createSparseGPUCodegenPass() {
</span></span><span style=display:flex><span>  <span style=color:#fe8019>return</span> std<span style=color:#fe8019>::</span>make_unique<span style=color:#fe8019>&lt;</span>SparseGPUCodegenPass<span style=color:#fe8019>&gt;</span>();
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>所有注册到 sparse_tensor pipeline 的 Pass</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-text data-lang=text><span style=display:flex><span>* LinalgGeneralization
</span></span><span style=display:flex><span>* SparsificationAndBufferization
</span></span><span style=display:flex><span>* Canoicalizer
</span></span><span style=display:flex><span>* FinalizingBufferize
</span></span><span style=display:flex><span>* (GPU Codegen: 只有在 sparse-compiler pipeline 里指定了 gpu-triple 才会用的一系列 Pass)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    * SparseGPUCodegenPass
</span></span><span style=display:flex><span>    * StripDebugInfoPass
</span></span><span style=display:flex><span>    * ConvertSCFTOCFPass
</span></span><span style=display:flex><span>    * LowerGpuOpsToNVVMOpsPass
</span></span><span style=display:flex><span>    * GpuToLLVMConversionPass
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>* ConvertLinalgToLoops
</span></span><span style=display:flex><span>* ConvertVectoToSCF
</span></span><span style=display:flex><span>* ConvertSCFToCF
</span></span><span style=display:flex><span>* ExapndStridedMetadata
</span></span><span style=display:flex><span>* LowerAffine
</span></span><span style=display:flex><span>* ConvertVectorToLLVM (With lowerVectorToLLVMOptions)
</span></span><span style=display:flex><span>* FinalizeMemRefToLLVM
</span></span><span style=display:flex><span>* ConvertComplexToStandard
</span></span><span style=display:flex><span>* ArithExpandOps
</span></span><span style=display:flex><span>* ConvertMathToLLVM
</span></span><span style=display:flex><span>* ConvertComplexToLibm
</span></span><span style=display:flex><span>* ConvertVectorToLLVM
</span></span><span style=display:flex><span>* ConvertComplexToLLVM
</span></span><span style=display:flex><span>* ReconcileUnrealizedCasts
</span></span></code></pre></div><p>而 <code>SparseCompilerOptions</code> 则是在 <code>mlir/include/mlir/Dialect/SparseTensor/Pipelines/Passes.h</code> 定义。
如果需要看详细可以用 <code>mlir-opt --help</code> 查看。需要关注 <code>SparseCompilerOptions</code> 提供了一个成员函数：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c++ data-lang=c++><span style=display:flex><span>ConvertVectorToLLVMPassOptions <span style=color:#fabd2f>lowerVectorToLLVMOptions</span>() <span style=color:#fe8019>const</span> {
</span></span><span style=display:flex><span>  ConvertVectorToLLVMPassOptions opts{};
</span></span><span style=display:flex><span>  opts.reassociateFPReductions <span style=color:#fe8019>=</span> reassociateFPReductions;
</span></span><span style=display:flex><span>  opts.force32BitVectorIndices <span style=color:#fe8019>=</span> force32BitVectorIndices;
</span></span><span style=display:flex><span>  opts.armNeon <span style=color:#fe8019>=</span> armNeon;
</span></span><span style=display:flex><span>  opts.armSVE <span style=color:#fe8019>=</span> armSVE;
</span></span><span style=display:flex><span>  opts.amx <span style=color:#fe8019>=</span> amx;
</span></span><span style=display:flex><span>  opts.x86Vector <span style=color:#fe8019>=</span> x86Vector;
</span></span><span style=display:flex><span>  <span style=color:#fe8019>return</span> opts;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h1 id=passes>Passes</h1><p>接下来来分析 Sparsity 以及向量化相关的 Pass。</p><h2 id=pass-sparsificationandbufferization>Pass: <code>SparsificationAndBufferization</code></h2><blockquote><p>mlir/lib/Dialect/SparseTensor/Transforms/SparsificationAndBufferizationPass.cpp</p></blockquote><p><code>SparsificationAndBufferization</code> pass 负责处理 Tensor 到 Memref 的 Lower。
拥有 Sparsity Attribute 的 Tensor 会被 Sparsification 专门处理并由专门处理
sparse_tensor Dialect 的 Pass 来 lower。</p><p><code>SparsificationAndBufferization</code> Pass 非常灵活，支持许多自定义选项。这些选项
都可以通过 <code>sparse-compiler</code> pass 传递进去。</p><p><code>SparsificationAndBufferization</code> 目前支持的 Options</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c++ data-lang=c++><span style=display:flex><span>SparsificationAndBufferizationPass(
</span></span><span style=display:flex><span>  <span style=color:#fe8019>const</span> bufferization<span style=color:#fe8019>::</span>OneShotBufferizationOptions <span style=color:#fe8019>&amp;</span>bufferizationOptions,
</span></span><span style=display:flex><span>  <span style=color:#fe8019>const</span> SparsificationOptions <span style=color:#fe8019>&amp;</span>sparsificationOptions,
</span></span><span style=display:flex><span>  <span style=color:#fe8019>const</span> SparseTensorConversionOptions <span style=color:#fe8019>&amp;</span>sparseTensorConversionOptions,
</span></span><span style=display:flex><span>  <span style=color:#fabd2f>bool</span> createSparseDeallocs,
</span></span><span style=display:flex><span>  <span style=color:#fabd2f>bool</span> enableRuntimeLibrary,
</span></span><span style=display:flex><span>  <span style=color:#fabd2f>bool</span> enableBufferInitialization,
</span></span><span style=display:flex><span>  <span style=color:#fabd2f>unsigned</span> vectorLength,
</span></span><span style=display:flex><span>  <span style=color:#fabd2f>bool</span> enableVLAVectorization,
</span></span><span style=display:flex><span>  <span style=color:#fabd2f>bool</span> enableSIMDIndex32
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>其中</p><p>Dense 的 Tensor 是从 <code>runDenseBufferization()</code> 函数走的普通 Bufferization 的
路径 lower。 这个函数会把所有带 Sparsity 属性的 Tensor 都过滤掉，只对 dense 的
operation 做 bufferization。</p><p>Sparse Tensor 则是通过函数 <code>runOnOperatoin()</code> lower，这个函数会跑三次 Pipeline：</p><h3 id=pipeline-1>Pipeline 1</h3><p>第一次跑 pipeline 主要是负责 sparse tensor 的 rewrite。在第一条 pipeline 里会创建一个新的 <code>OpPassManager</code>，
其中加入 <code>PreSparsificationRewritePass</code> 和 <code>EmptyTensorToAllocTensorPass</code>。</p><h4 id=presparsificationrewritepass>PreSparsificationRewritePass</h4><p>文件位置：mlir/lib/Dialect/SparseTensor/Transform/SparseTensorPasses.cpp</p><p><code>PreSparsificationRewritePass</code> 负责处理重写 sparse tensor，像一些转换 dense tensor 到 sparse tensor，
重塑 sparse tensor 等。其主要往 <code>RewritePattenSet</code> 里加入
<code>FoldInvariantYield, FuseSparseMultiplyOverAdd, FuseTensorCast</code> 三个 <code>OpRewritePattern</code>。
其中 <code>FoldInvariantYield</code> 负责优化 sparse tensor 里的零值，
<code>FuseSparseMultiplyOverAdd</code> 负责合并乘积和加的操作，比如：</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-text data-lang=text><span style=display:flex><span>T(i,j) = SUM(k, A(i,j,k) * B(i,j,k) * ... )
</span></span><span style=display:flex><span>X(i,j) = S(i,j) * T(i,j)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>// After FuseSparseMultiplyOverAdd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X(i,j) = SUM(k, S(i,j) * A(i,j,k) * B(i,j,k) * ... )
</span></span></code></pre></div><p>而 <code>FuseTensorCast</code> 负责将 tensor 类型转换操作优化成直接的类型覆写。
其负责三种 rewrite：</p><ol><li>消除无意义的 Type cast</li></ol><p>如果在使用 tensor.cast 的时候，cast 操作两边的类型完全相同，那么 FuseTensorCast 就会直接把这些 cast
全部优化掉。</p><p>以下的代码会被完全优化掉</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mlir data-lang=mlir><span style=display:flex><span>%0 = <span style=color:#fabd2f>tensor</span>.cast %a : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>, #SparseVector&gt; to <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>, #SparseVector&gt;
</span></span><span style=display:flex><span>%1 = <span style=color:#fabd2f>tensor</span>.cast %0 : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>, #SparseVector&gt; to <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>, #SparseVector&gt;
</span></span></code></pre></div><ol start=2><li>消除多次 tensor cast</li></ol><p>如果忽视 sparse 的属性之后，<code>tensor.cast</code> 的源类型和目标类型是完全相同的，则这个 tensor.cast 操作会被消除掉，
然后前一个操作产生的 tensor 类型属性会被修改成目标类型的属性。</p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mlir data-lang=mlir><span style=display:flex><span><span style=color:#928374;font-style:italic>// Before
</span></span></span><span style=display:flex><span><span style=color:#928374;font-style:italic></span>%extracted_slice = <span style=color:#fabd2f>tensor</span>.extract_slice %a[<span style=color:#d3869b>1</span>, <span style=color:#d3869b>0</span>] [<span style=color:#d3869b>1</span>, <span style=color:#d3869b>3</span>] [<span style=color:#d3869b>1</span>, <span style=color:#d3869b>1</span>] : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>2x3x</span><span style=color:#fe8019>i64</span>, #SortedCOO&gt; to <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>1x3x</span><span style=color:#fe8019>i64</span>&gt;
</span></span><span style=display:flex><span>%cast = <span style=color:#fabd2f>tensor</span>.cast %extracted_slice : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>1x3x</span><span style=color:#fe8019>i64</span>&gt; to <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>1x3x</span><span style=color:#fe8019>i64</span>, #Slice&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#928374;font-style:italic>// After
</span></span></span><span style=display:flex><span><span style=color:#928374;font-style:italic></span>%extracted_slice = <span style=color:#fabd2f>tensor</span>.extract_slice %a[<span style=color:#d3869b>1</span>, <span style=color:#d3869b>0</span>] [<span style=color:#d3869b>1</span>, <span style=color:#d3869b>3</span>] [<span style=color:#d3869b>1</span>, <span style=color:#d3869b>1</span>] : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>2x3x</span><span style=color:#fe8019>i64</span>, #SortedCOO&gt; to <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>1x3x</span><span style=color:#fe8019>i64</span>, #Slice&gt;
</span></span></code></pre></div><ol start=3><li>修复错误的 tensor.cast 使用</li></ol><p>如果用 tensor.cast 的任意一个操作数有 Sparse 的属性，<code>FuseTensorCast</code> 会把这个 operation 换回
<code>sparse_tensor.convert</code></p><div class=highlight><pre tabindex=0 style=color:#ebdbb2;background-color:#282828;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mlir data-lang=mlir><span style=display:flex><span><span style=color:#928374;font-style:italic>// Before
</span></span></span><span style=display:flex><span><span style=color:#928374;font-style:italic></span>%0 = <span style=color:#fabd2f>tensor</span>.cast %a : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt; to <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>, #SparseVector&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#928374;font-style:italic>// After
</span></span></span><span style=display:flex><span><span style=color:#928374;font-style:italic></span>%0 = sparse_tensor.convert %a : <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>&gt; to <span style=color:#fabd2f>tensor</span>&lt;<span style=color:#d3869b>?x</span><span style=color:#fe8019>f32</span>, #SparseVector&gt;
</span></span></code></pre></div><h3 id=pipeline-2>Pipeline 2</h3><p>第二条 Pipeline 负责对 Tensor 做 Sparse Bufferization 操作。
在这条 Pipeline 里，默认会加入 <code>SparsificationPass</code> 和 <code>PostSparsificationRewritePass</code>。
根据 <code>vl</code> 是否在 <code>sparse-compiler</code> pass 里设置了大于零的值，还会可选的加入 <code>createLoopInvariantCodeMotionPass</code>
和 <code>SparseVectorizationPass</code>。</p><p>除此之外，根据 <code>RuntimeLibrary</code> 选项是否启用，PassManager 还会可选的加入一部分 Pass。
如果启用了 RuntimeLibrary，则只有 <code>SparseTensorConversionPass</code> 会被加入 <code>OpPassManager</code>，
如果没启用，那么会加入 codegen pass <code>SparseTensorCodegenPass</code>，还有 <code>SparseBufferRewritePass</code>
和 <code>StorageSpecifierToLLVMPass</code>。</p><h3 id=pipeline-3>Pipeline 3</h3><p>第三条 Pipe line 则是对剩余的 Dense tensor 做 bufferization 操作。</p></section><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]"><a class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://blog.sh1mar.in/post/mlir/tensor/><span class=mr-1.5>←</span><span>Tensor concept</span></a>
<a class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://blog.sh1mar.in/post/mlir/mlir-opt/><span>How mlir-opt work</span><span class=ml-1.5>→</span></a></nav></article></main><footer class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"><div class=mr-auto>&copy; 2024
<a class=link href=https://blog.sh1mar.in>sh1marin's blog</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>Powered by Hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>✎ Paper</a></footer></body></html>